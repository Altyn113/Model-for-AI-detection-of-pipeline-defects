{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–∏—Å—Ç–µ–º–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ Random Forest\n",
    "\n",
    "**–ê–≤—Ç–æ—Ä:** –ö–æ–Ω–¥—Ä–∞—à–æ–≤ –î.–í.\n",
    "**–ù–∞—É—á–Ω—ã–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å:** –ó–∞—Ä—É–±–∏–Ω –ê.–ì. (–∫.—Ö.–Ω., –¥–æ—Ü–µ–Ω—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è –Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤–æ–≥–æ –¥–µ–ª–∞ –ò–®–ü–† –¢–ü–£)\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º (–°–¢–°) –Ω–∞ –æ–±—ä–µ–∫—Ç–∞—Ö –º–∞–≥–∏—Å—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ –Ω–µ—Ñ—Ç–∏ –∏ –Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\n",
    "- üéØ **–ê–ª–≥–æ—Ä–∏—Ç–º**: Random Forest –¥–ª—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏\n",
    "- üìä **–î–∞–Ω–Ω—ã–µ**: –°–∏–º—É–ª—è—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ MIMII –æ—Ç Hitachi —Å –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "- üîç **–ó–∞–¥–∞—á–∞**: –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–Ω–æ—Ä–º–∞/–∞–Ω–æ–º–∞–ª–∏—è)\n",
    "- ‚ö° **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: 99.9% —Ç–æ—á–Ω–æ—Å—Ç—å, ROC-AUC = 1.000\n",
    "- üè≠ **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**: –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, accuracy_score,\n",
    "                            confusion_matrix, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "from typing import Tuple, Dict, Any\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "print(f\"üìã NumPy: {np.__version__}\")\n",
    "print(f\"üìã Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineDiagnosticSystem:\n",
    "    \"\"\"\n",
    "    –°–∏—Å—Ç–µ–º–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º\n",
    "    –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (Random Forest)\n",
    "    \n",
    "    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏–∑ —Ç–µ–∑–∏—Å–æ–≤ –ö–æ–Ω–¥—Ä–∞—à–æ–≤–∞ –î.–í. –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ\n",
    "    —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π —Å –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–µ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_selector = None\n",
    "        self.training_history = {}\n",
    "        \n",
    "    def generate_mimii_like_data(self, n_samples: int = 5000) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç MIMII –æ—Ç Hitachi\n",
    "        –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "        \n",
    "        Args:\n",
    "            n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "            \n",
    "        Returns:\n",
    "            features: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (MFCC, —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)\n",
    "            labels: –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (0 - –Ω–æ—Ä–º–∞, 1 - –∞–Ω–æ–º–∞–ª–∏—è)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # –°–∏–º—É–ª—è—Ü–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è\n",
    "        n_features = 40\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ (60%)\n",
    "        n_normal = int(n_samples * 0.6)\n",
    "        normal_features = np.random.normal(0, 1, (n_normal, n_features))\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ (40%)\n",
    "        n_anomalous = n_samples - n_normal\n",
    "        anomalous_features = np.random.normal(0, 2.5, (n_anomalous, n_features))\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –≤ MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã\n",
    "        anomalous_features[:, :10] += np.random.normal(3, 1, (n_anomalous, 10))\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "        features = np.vstack([normal_features, anomalous_features])\n",
    "        labels = np.hstack([np.zeros(n_normal), np.ones(n_anomalous)])\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        feature_names = [f'mfcc_{i+1}' for i in range(13)] + \\\n",
    "                       [f'spectral_centroid_{i+1}' for i in range(10)] + \\\n",
    "                       [f'spectral_rolloff_{i+1}' for i in range(10)] + \\\n",
    "                       [f'zero_crossing_rate_{i+1}' for i in range(7)]\n",
    "        \n",
    "        features_df = pd.DataFrame(features, columns=feature_names)\n",
    "        labels_series = pd.Series(labels, name='anomaly')\n",
    "        \n",
    "        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "        indices = np.random.permutation(len(features_df))\n",
    "        features_df = features_df.iloc[indices].reset_index(drop=True)\n",
    "        labels_series = labels_series.iloc[indices].reset_index(drop=True)\n",
    "        \n",
    "        return features_df, labels_series\n",
    "    \n",
    "    def preprocess_data(self, features: pd.DataFrame, labels: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        \n",
    "        Args:\n",
    "            features: –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "            labels: –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "            \n",
    "        Returns:\n",
    "            processed_features: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "            encoded_labels: –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "        \"\"\"\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "        \n",
    "        # –û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ø 25)\n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=25)\n",
    "        selected_features = self.feature_selector.fit_transform(scaled_features_df, labels)\n",
    "        \n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        selected_indices = self.feature_selector.get_support(indices=True)\n",
    "        selected_names = [features.columns[i] for i in selected_indices]\n",
    "        \n",
    "        processed_features = pd.DataFrame(selected_features, columns=selected_names)\n",
    "        encoded_labels = labels\n",
    "        \n",
    "        return processed_features, encoded_labels\n",
    "    \n",
    "    def train_model(self, features: pd.DataFrame, labels: pd.Series, \n",
    "                   test_size: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "        \n",
    "        Args:\n",
    "            features: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "            labels: –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "            test_size: —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            \n",
    "        Returns:\n",
    "            training_results: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "        \"\"\"\n",
    "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, labels, test_size=test_size, \n",
    "            random_state=self.random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Random Forest —Å–æ–≥–ª–∞—Å–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,           # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "            max_depth=15,               # –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "            min_samples_split=5,        # –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "            min_samples_leaf=2,         # –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ\n",
    "            max_features='sqrt',        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –¥–µ—Ä–µ–≤–æ\n",
    "            bootstrap=True,             # Bootstrap –≤—ã–±–æ—Ä–∫–∞\n",
    "            class_weight='balanced',    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1                   # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "        )\n",
    "        \n",
    "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "        y_test_pred = self.model.predict(X_test)\n",
    "        y_test_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        cv_scores = cross_val_score(\n",
    "            self.model, X_train, y_train, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
    "            scoring='f1'\n",
    "        )\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "        self.training_history = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_test_proba),\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'feature_importance': dict(zip(features.columns, self.model.feature_importances_)),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_test_pred),\n",
    "            'classification_report': classification_report(y_test, y_test_pred, output_dict=True),\n",
    "            'test_predictions': y_test_pred,\n",
    "            'test_probabilities': y_test_proba,\n",
    "            'test_labels': y_test.values,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def predict_anomaly(self, features: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        \n",
    "        Args:\n",
    "            features: –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "            \n",
    "        Returns:\n",
    "            predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (0/1)\n",
    "            probabilities: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–π [0-1]\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–ø–æ–ª–Ω–∏—Ç–µ train_model() —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–π –∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        scaled_features = self.scaler.transform(features)\n",
    "        selected_features = self.feature_selector.transform(scaled_features)\n",
    "        \n",
    "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "        predictions = self.model.predict(selected_features)\n",
    "        probabilities = self.model.predict_proba(selected_features)[:, 1]\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_feature_importance(self, top_n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞.\")\n",
    "        \n",
    "        importance_dict = self.training_history['feature_importance']\n",
    "        importance_df = pd.DataFrame(\n",
    "            list(importance_dict.items()),\n",
    "            columns=['feature', 'importance']\n",
    "        ).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def print_diagnostic_report(self) -> None:\n",
    "        \"\"\"–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Å–∏—Å—Ç–µ–º–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîç –û–¢–ß–ï–¢ –°–ò–°–¢–ï–ú–´ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò –¢–†–£–ë–û–ü–†–û–í–û–î–û–í\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {self.training_history['train_size']}\")\n",
    "        print(f\"üß™ –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {self.training_history['test_size']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üìà –ú–ï–¢–†–ò–ö–ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:\")\n",
    "        print(f\"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {self.training_history['train_accuracy']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {self.training_history['test_accuracy']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ ROC-AUC Score: {self.training_history['test_roc_auc']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (F1): {self.training_history['cv_mean']:.4f} ¬± {self.training_history['cv_std']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üéØ –î–ï–¢–ê–õ–¨–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø:\")\n",
    "        cr = self.training_history['classification_report']\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "        class_keys = [k for k in cr.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "        \n",
    "        for class_key in sorted(class_keys):\n",
    "            if str(class_key) in ['0', '0.0'] or class_key == 0.0:\n",
    "                class_name, class_label = \"–ù–æ—Ä–º–∞\", \"0\"\n",
    "            elif str(class_key) in ['1', '1.0'] or class_key == 1.0:\n",
    "                class_name, class_label = \"–ê–Ω–æ–º–∞–ª–∏—è\", \"1\"\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   {class_name} (–∫–ª–∞—Å—Å {class_label}):\")\n",
    "            print(f\"     - –¢–æ—á–Ω–æ—Å—Ç—å: {cr[class_key]['precision']:.4f}\")\n",
    "            print(f\"     - –ü–æ–ª–Ω–æ—Ç–∞: {cr[class_key]['recall']:.4f}\")\n",
    "            print(f\"     - F1-–º–µ—Ä–∞: {cr[class_key]['f1-score']:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"üîß –¢–û–ü-5 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:\")\n",
    "        top_features = self.get_feature_importance(5)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"‚úÖ –ö–ª–∞—Å—Å PipelineDiagnosticSystem —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã\n",
    "\n",
    "### –≠—Ç–∞–ø 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "print(\"üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò –¢–†–£–ë–û–ü–†–û–í–û–î–û–í\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "diagnostic_system = PipelineDiagnosticSystem(random_state=42)\n",
    "print(\"‚úÖ –°–∏—Å—Ç–µ–º–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "print()\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç MIMII\n",
    "print(\"üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• (–∏–º–∏—Ç–∞—Ü–∏—è MIMII dataset –æ—Ç Hitachi)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "features, labels = diagnostic_system.generate_mimii_like_data(n_samples=5000)\n",
    "\n",
    "print(f\"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(features)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
    "print(f\"üìè –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape[1]}\")\n",
    "print(f\"üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
    "print(f\"   ‚Ä¢ –ù–æ—Ä–º–∞ (0): {len(labels[labels == 0])} –æ–±—Ä–∞–∑—Ü–æ–≤ ({len(labels[labels == 0])/len(labels)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ –ê–Ω–æ–º–∞–ª–∏—è (1): {len(labels[labels == 1])} –æ–±—Ä–∞–∑—Ü–æ–≤ ({len(labels[labels == 1])/len(labels)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üìà –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–• (–ø–µ—Ä–≤—ã–µ 5 –æ–±—Ä–∞–∑—Ü–æ–≤, –ø–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):\")\n",
    "display(features.iloc[:5, :10])\n",
    "\n",
    "print(\"\\nüî¢ –û–ü–ò–°–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê (–ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):\")\n",
    "display(features.describe().iloc[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "processed_features, processed_labels = diagnostic_system.preprocess_data(features, labels)\n",
    "\n",
    "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
    "print(f\"üìè –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞: {processed_features.shape[1]}\")\n",
    "print(f\"üéØ –û—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–µ—Ä–≤—ã–µ 10):\")\n",
    "for i, feature in enumerate(processed_features.columns[:10], 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "if len(processed_features.columns) > 10:\n",
    "    print(f\"   ... –∏ –µ—â–µ {len(processed_features.columns) - 10} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest\n",
    "print(\"üéì –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò RANDOM FOREST\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "training_results = diagnostic_system.train_model(processed_features, processed_labels)\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!\")\n",
    "print()\n",
    "\n",
    "# –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\n",
    "diagnostic_system.print_diagnostic_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "print(\"üîÆ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ù–û–í–´–• –î–ê–ù–ù–´–•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "new_features, new_labels = diagnostic_system.generate_mimii_like_data(n_samples=500)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π\n",
    "predictions, probabilities = diagnostic_system.predict_anomaly(new_features)\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "correct_predictions = np.sum(predictions == new_labels.values)\n",
    "accuracy_new_data = correct_predictions / len(predictions)\n",
    "\n",
    "print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_features)} –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
    "print(f\"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy_new_data:.4f}\")\n",
    "print()\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(\"üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\")\n",
    "for class_val, count in zip(unique, counts):\n",
    "    class_name = \"–ù–æ—Ä–º–∞\" if class_val == 0 else \"–ê–Ω–æ–º–∞–ª–∏—è\"\n",
    "    percentage = (count / len(predictions)) * 100\n",
    "    print(f\"   ‚Ä¢ {class_name} (–∫–ª–∞—Å—Å {int(class_val)}): {count} –æ–±—Ä–∞–∑—Ü–æ–≤ ({percentage:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏\n",
    "high_confidence = np.sum((probabilities > 0.8) | (probabilities < 0.2))\n",
    "medium_confidence = np.sum((probabilities >= 0.2) & (probabilities <= 0.8))\n",
    "\n",
    "print(\"üé≤ –ê–ù–ê–õ–ò–ó –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ú–û–î–ï–õ–ò:\")\n",
    "print(f\"   ‚Ä¢ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>80% –∏–ª–∏ <20%): {high_confidence} –æ–±—Ä–∞–∑—Ü–æ–≤ ({high_confidence/len(predictions)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (20-80%): {medium_confidence} –æ–±—Ä–∞–∑—Ü–æ–≤ ({medium_confidence/len(predictions)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "print(\"üìã –ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô (–ø–µ—Ä–≤—ã–µ 10):\")\n",
    "print(\"‚Ññ  | –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(min(10, len(predictions))):\n",
    "    true_class = \"–ù–æ—Ä–º–∞\" if new_labels.iloc[i] == 0 else \"–ê–Ω–æ–º–∞–ª–∏—è\"\n",
    "    pred_class = \"–ù–æ—Ä–º–∞\" if predictions[i] == 0 else \"–ê–Ω–æ–º–∞–ª–∏—è\"\n",
    "    prob = probabilities[i]\n",
    "    confidence = \"–í—ã—Å–æ–∫–∞—è\" if prob > 0.8 or prob < 0.2 else \"–°—Ä–µ–¥–Ω—è—è\"\n",
    "    correct = \"‚úì\" if predictions[i] == new_labels.iloc[i] else \"‚úó\"\n",
    "    print(f\"{i+1:2d} | {true_class:13s} | {pred_class:12s} | {prob:17.4f} | {confidence:10s} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –≠—Ç–∞–ø 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 1: –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "plt.subplot(2, 2, 1)\n",
    "top_features = diagnostic_system.get_feature_importance(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], \n",
    "         color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')\n",
    "plt.title('–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 2: –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n",
    "plt.subplot(2, 2, 2)\n",
    "cm = diagnostic_system.training_history['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['–ù–æ—Ä–º–∞', '–ê–Ω–æ–º–∞–ª–∏—è'], \n",
    "            yticklabels=['–ù–æ—Ä–º–∞', '–ê–Ω–æ–º–∞–ª–∏—è'])\n",
    "plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\n",
    "plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\n",
    "plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "plt.subplot(2, 2, 3)\n",
    "test_probs = diagnostic_system.training_history['test_probabilities']\n",
    "test_labels = diagnostic_system.training_history['test_labels']\n",
    "\n",
    "plt.hist(test_probs[test_labels == 0], bins=30, alpha=0.7, label='–ù–æ—Ä–º–∞', color='blue')\n",
    "plt.hist(test_probs[test_labels == 1], bins=30, alpha=0.7, label='–ê–Ω–æ–º–∞–ª–∏—è', color='red')\n",
    "plt.xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')\n",
    "plt.legend()\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 4: ROC –∫—Ä–∏–≤–∞—è\n",
    "plt.subplot(2, 2, 4)\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
    "auc_score = diagnostic_system.training_history['test_roc_auc']\n",
    "\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC –∫—Ä–∏–≤–∞—è (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='–°–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('–õ–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è')\n",
    "plt.ylabel('–ò—Å—Ç–∏–Ω–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è')\n",
    "plt.title('ROC –∫—Ä–∏–≤–∞—è')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('–°–∏—Å—Ç–µ–º–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–æ–≤ - –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤', \n",
    "             fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –∏ –≤—ã–≤–æ–¥—ã\n",
    "\n",
    "### –û—Å–Ω–æ–≤–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:\n",
    "\n",
    "‚úÖ **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å**: 99.9% –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö  \n",
    "‚úÖ **–ò–¥–µ–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: ROC-AUC = 1.000  \n",
    "‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞**: F1-–º–µ—Ä–∞ 0.9991 ¬± 0.0008 (–∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è)  \n",
    "‚úÖ **–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å**: 98.8% –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é  \n",
    "\n",
    "### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:\n",
    "\n",
    "- **–ê–ª–≥–æ—Ä–∏—Ç–º**: Random Forest (100 –¥–µ—Ä–µ–≤—å–µ–≤) —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "- **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞**: StandardScaler + SelectKBest (25 –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
    "- **–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: 5-fold —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "\n",
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:\n",
    "\n",
    "1. üè≠ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏** —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è\n",
    "2. üö® **–†–∞–Ω–Ω–µ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π** –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∞–≤–∞—Ä–∏–π\n",
    "3. üìà **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–ª–∞–Ω–æ–≤ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è** –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "4. üí∞ **–°–Ω–∏–∂–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç** –Ω–∞ –≤–Ω–µ–ø–ª–∞–Ω–æ–≤—ã–µ —Ä–µ–º–æ–Ω—Ç—ã\n",
    "5. üõ°Ô∏è **–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏** –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
    "\n",
      "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è:\n",
    "\n",
    "- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞—Ç—á–∏–∫–∞–º–∏ MIMII –æ—Ç Hitachi\n",
    "- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ\n",
    "- –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∞–ª–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤\n",
    "- –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
