{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Random Forest\n",
    "\n",
    "**ÐÐ²Ñ‚Ð¾Ñ€:** ÐšÐ¾Ð½Ð´Ñ€Ð°ÑˆÐ¾Ð² Ð”.Ð’.\n",
    "**ÐÐ°ÑƒÑ‡Ð½Ñ‹Ð¹ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ:** Ð—Ð°Ñ€ÑƒÐ±Ð¸Ð½ Ð.Ð“. (Ðº.Ñ….Ð½., Ð´Ð¾Ñ†ÐµÐ½Ñ‚ Ð¾Ñ‚Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½ÐµÑ„Ñ‚ÐµÐ³Ð°Ð·Ð¾Ð²Ð¾Ð³Ð¾ Ð´ÐµÐ»Ð° Ð˜Ð¨ÐŸÐ  Ð¢ÐŸÐ£)\n",
    "\n",
    "## ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°\n",
    "\n",
    "Ð”Ð°Ð½Ð½Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð° Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ (Ð¡Ð¢Ð¡) Ð½Ð° Ð¾Ð±ÑŠÐµÐºÑ‚Ð°Ñ… Ð¼Ð°Ð³Ð¸ÑÑ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð½ÐµÑ„Ñ‚Ð¸ Ð¸ Ð½ÐµÑ„Ñ‚ÐµÐ¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.\n",
    "\n",
    "### ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸:\n",
    "- ðŸŽ¯ **ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼**: Random Forest Ð´Ð»Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚Ð¸\n",
    "- ðŸ“Š **Ð”Ð°Ð½Ð½Ñ‹Ðµ**: Ð¡Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° MIMII Ð¾Ñ‚ Hitachi Ñ Ð°ÐºÑƒÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸\n",
    "- ðŸ” **Ð—Ð°Ð´Ð°Ñ‡Ð°**: Ð‘Ð¸Ð½Ð°Ñ€Ð½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ (Ð½Ð¾Ñ€Ð¼Ð°/Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ)\n",
    "- âš¡ **ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ**: 99.9% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ, ROC-AUC = 1.000\n",
    "- ðŸ­ **ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ**: ÐŸÑ€ÐµÐ´Ð¸ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐº\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, accuracy_score,\n",
    "                            confusion_matrix, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "from typing import Tuple, Dict, Any\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹\")\n",
    "print(f\"ðŸ“‹ NumPy: {np.__version__}\")\n",
    "print(f\"ðŸ“‹ Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineDiagnosticSystem:\n",
    "    \"\"\"\n",
    "    Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼\n",
    "    Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Random Forest)\n",
    "    \n",
    "    Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ Ð¸Ð· Ñ‚ÐµÐ·Ð¸ÑÐ¾Ð² ÐšÐ¾Ð½Ð´Ñ€Ð°ÑˆÐ¾Ð²Ð° Ð”.Ð’. Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾\n",
    "    Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð½ÐµÐ¸ÑÐ¿Ñ€Ð°Ð²Ð½Ð¾ÑÑ‚ÐµÐ¹ Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð°.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42):\n",
    "        \"\"\"Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸\"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_selector = None\n",
    "        self.training_history = {}\n",
    "        \n",
    "    def generate_mimii_like_data(self, n_samples: int = 5000) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¸Ð¼Ð¸Ñ‚Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ MIMII Ð¾Ñ‚ Hitachi\n",
    "        Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸\n",
    "        \n",
    "        Args:\n",
    "            n_samples: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "            \n",
    "        Returns:\n",
    "            features: Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² (MFCC, ÑÐ¿ÐµÐºÑ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸)\n",
    "            labels: Ð¼ÐµÑ‚ÐºÐ¸ ÐºÐ»Ð°ÑÑÐ¾Ð² (0 - Ð½Ð¾Ñ€Ð¼Ð°, 1 - Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Ð¡Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ Ð°ÐºÑƒÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð¿Ñ€Ð¾Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ\n",
    "        n_features = 40\n",
    "        \n",
    "        # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² (60%)\n",
    "        n_normal = int(n_samples * 0.6)\n",
    "        normal_features = np.random.normal(0, 1, (n_normal, n_features))\n",
    "        \n",
    "        # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð°Ð½Ð¾Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² (40%)\n",
    "        n_anomalous = n_samples - n_normal\n",
    "        anomalous_features = np.random.normal(0, 2.5, (n_anomalous, n_features))\n",
    "        \n",
    "        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹ Ð² MFCC ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚Ñ‹\n",
    "        anomalous_features[:, :10] += np.random.normal(3, 1, (n_anomalous, 10))\n",
    "        \n",
    "        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "        features = np.vstack([normal_features, anomalous_features])\n",
    "        labels = np.hstack([np.zeros(n_normal), np.ones(n_anomalous)])\n",
    "        \n",
    "        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾ÑÐ¼Ñ‹ÑÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ð¹ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "        feature_names = [f'mfcc_{i+1}' for i in range(13)] + \\\n",
    "                       [f'spectral_centroid_{i+1}' for i in range(10)] + \\\n",
    "                       [f'spectral_rolloff_{i+1}' for i in range(10)] + \\\n",
    "                       [f'zero_crossing_rate_{i+1}' for i in range(7)]\n",
    "        \n",
    "        features_df = pd.DataFrame(features, columns=feature_names)\n",
    "        labels_series = pd.Series(labels, name='anomaly')\n",
    "        \n",
    "        # ÐŸÐµÑ€ÐµÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "        indices = np.random.permutation(len(features_df))\n",
    "        features_df = features_df.iloc[indices].reset_index(drop=True)\n",
    "        labels_series = labels_series.iloc[indices].reset_index(drop=True)\n",
    "        \n",
    "        return features_df, labels_series\n",
    "    \n",
    "    def preprocess_data(self, features: pd.DataFrame, labels: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…: Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ñ‚Ð±Ð¾Ñ€ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "        \n",
    "        Args:\n",
    "            features: Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "            labels: Ð¼ÐµÑ‚ÐºÐ¸ ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
    "            \n",
    "        Returns:\n",
    "            processed_features: Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "            encoded_labels: Ð·Ð°ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸\n",
    "        \"\"\"\n",
    "        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "        \n",
    "        # ÐžÑ‚Ð±Ð¾Ñ€ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² (Ñ‚Ð¾Ð¿ 25)\n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=25)\n",
    "        selected_features = self.feature_selector.fit_transform(scaled_features_df, labels)\n",
    "        \n",
    "        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ð¹ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "        selected_indices = self.feature_selector.get_support(indices=True)\n",
    "        selected_names = [features.columns[i] for i in selected_indices]\n",
    "        \n",
    "        processed_features = pd.DataFrame(selected_features, columns=selected_names)\n",
    "        encoded_labels = labels\n",
    "        \n",
    "        return processed_features, encoded_labels\n",
    "    \n",
    "    def train_model(self, features: pd.DataFrame, labels: pd.Series, \n",
    "                   test_size: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Random Forest Ñ ÐºÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹\n",
    "        \n",
    "        Args:\n",
    "            features: Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
    "            labels: Ð¼ÐµÑ‚ÐºÐ¸ ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
    "            test_size: Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸\n",
    "            \n",
    "        Returns:\n",
    "            training_results: ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸\n",
    "        \"\"\"\n",
    "        # Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²ÑƒÑŽ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, labels, test_size=test_size, \n",
    "            random_state=self.random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Random Forest ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,           # ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´ÐµÑ€ÐµÐ²ÑŒÐµÐ²\n",
    "            max_depth=15,               # ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
    "            min_samples_split=5,        # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ\n",
    "            min_samples_leaf=2,         # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² Ð² Ð»Ð¸ÑÑ‚Ðµ\n",
    "            max_features='sqrt',        # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð½Ð° Ð´ÐµÑ€ÐµÐ²Ð¾\n",
    "            bootstrap=True,             # Bootstrap Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ°\n",
    "            class_weight='balanced',    # Ð‘Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1                   # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°\n",
    "        )\n",
    "        \n",
    "        # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "        y_test_pred = self.model.predict(X_test)\n",
    "        y_test_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # ÐšÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸\n",
    "        cv_scores = cross_val_score(\n",
    "            self.model, X_train, y_train, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
    "            scoring='f1'\n",
    "        )\n",
    "        \n",
    "        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
    "        self.training_history = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_test_proba),\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'feature_importance': dict(zip(features.columns, self.model.feature_importances_)),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_test_pred),\n",
    "            'classification_report': classification_report(y_test, y_test_pred, output_dict=True),\n",
    "            'test_predictions': y_test_pred,\n",
    "            'test_probabilities': y_test_proba,\n",
    "            'test_labels': y_test.values,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def predict_anomaly(self, features: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹ Ð´Ð»Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "        \n",
    "        Args:\n",
    "            features: Ð½Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ»Ð°ÑÑÑ‹ (0/1)\n",
    "            probabilities: Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹ [0-1]\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð°. Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ train_model() ÑÐ½Ð°Ñ‡Ð°Ð»Ð°.\")\n",
    "        \n",
    "        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð¹ Ð¶Ðµ Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸\n",
    "        scaled_features = self.scaler.transform(features)\n",
    "        selected_features = self.feature_selector.transform(scaled_features)\n",
    "        \n",
    "        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ\n",
    "        predictions = self.model.predict(selected_features)\n",
    "        probabilities = self.model.predict_proba(selected_features)[:, 1]\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_feature_importance(self, top_n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð°.\")\n",
    "        \n",
    "        importance_dict = self.training_history['feature_importance']\n",
    "        importance_df = pd.DataFrame(\n",
    "            list(importance_dict.items()),\n",
    "            columns=['feature', 'importance']\n",
    "        ).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def print_diagnostic_report(self) -> None:\n",
    "        \"\"\"Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"âŒ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð°. Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ.\")\n",
    "            return\n",
    "        \n",
    "        print(\"ðŸ” ÐžÐ¢Ð§Ð•Ð¢ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐ« Ð”Ð˜ÐÐ“ÐÐžÐ¡Ð¢Ð˜ÐšÐ˜ Ð¢Ð Ð£Ð‘ÐžÐŸÐ ÐžÐ’ÐžÐ”ÐžÐ’\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ðŸ“Š Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸: {self.training_history['train_size']}\")\n",
    "        print(f\"ðŸ§ª Ð Ð°Ð·Ð¼ÐµÑ€ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸: {self.training_history['test_size']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ðŸ“ˆ ÐœÐ•Ð¢Ð Ð˜ÐšÐ˜ ÐŸÐ ÐžÐ˜Ð—Ð’ÐžÐ”Ð˜Ð¢Ð•Ð›Ð¬ÐÐžÐ¡Ð¢Ð˜:\")\n",
    "        print(f\"   â€¢ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐµ: {self.training_history['train_accuracy']:.4f}\")\n",
    "        print(f\"   â€¢ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐµ: {self.training_history['test_accuracy']:.4f}\")\n",
    "        print(f\"   â€¢ ROC-AUC Score: {self.training_history['test_roc_auc']:.4f}\")\n",
    "        print(f\"   â€¢ ÐšÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ (F1): {self.training_history['cv_mean']:.4f} Â± {self.training_history['cv_std']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ðŸŽ¯ Ð”Ð•Ð¢ÐÐ›Ð¬ÐÐÐ¯ ÐšÐ›ÐÐ¡Ð¡Ð˜Ð¤Ð˜ÐšÐÐ¦Ð˜Ð¯:\")\n",
    "        cr = self.training_history['classification_report']\n",
    "        \n",
    "        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
    "        class_keys = [k for k in cr.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "        \n",
    "        for class_key in sorted(class_keys):\n",
    "            if str(class_key) in ['0', '0.0'] or class_key == 0.0:\n",
    "                class_name, class_label = \"ÐÐ¾Ñ€Ð¼Ð°\", \"0\"\n",
    "            elif str(class_key) in ['1', '1.0'] or class_key == 1.0:\n",
    "                class_name, class_label = \"ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ\", \"1\"\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   {class_name} (ÐºÐ»Ð°ÑÑ {class_label}):\")\n",
    "            print(f\"     - Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {cr[class_key]['precision']:.4f}\")\n",
    "            print(f\"     - ÐŸÐ¾Ð»Ð½Ð¾Ñ‚Ð°: {cr[class_key]['recall']:.4f}\")\n",
    "            print(f\"     - F1-Ð¼ÐµÑ€Ð°: {cr[class_key]['f1-score']:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"ðŸ”§ Ð¢ÐžÐŸ-5 Ð’ÐÐ–ÐÐ«Ð¥ ÐŸÐ Ð˜Ð—ÐÐÐšÐžÐ’:\")\n",
    "        top_features = self.get_feature_importance(5)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"âœ… ÐšÐ»Ð°ÑÑ PipelineDiagnosticSystem ÑÐ¾Ð·Ð´Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹\n",
    "\n",
    "### Ð­Ñ‚Ð°Ð¿ 1: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸\n",
    "print(\"ðŸš€ Ð˜ÐÐ˜Ð¦Ð˜ÐÐ›Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐ« Ð”Ð˜ÐÐ“ÐÐžÐ¡Ð¢Ð˜ÐšÐ˜ Ð¢Ð Ð£Ð‘ÐžÐŸÐ ÐžÐ’ÐžÐ”ÐžÐ’\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "diagnostic_system = PipelineDiagnosticSystem(random_state=42)\n",
    "print(\"âœ… Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð°\")\n",
    "print()\n",
    "\n",
    "# Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¸Ð¼Ð¸Ñ‚Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ MIMII\n",
    "print(\"ðŸ“Š Ð“Ð•ÐÐ•Ð ÐÐ¦Ð˜Ð¯ Ð”ÐÐÐÐ«Ð¥ (Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ MIMII dataset Ð¾Ñ‚ Hitachi)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "features, labels = diagnostic_system.generate_mimii_like_data(n_samples=5000)\n",
    "\n",
    "print(f\"âœ… Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {len(features)} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²\")\n",
    "print(f\"ðŸ“ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²: {features.shape[1]}\")\n",
    "print(f\"ðŸŽ¯ Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ¾Ð²:\")\n",
    "print(f\"   â€¢ ÐÐ¾Ñ€Ð¼Ð° (0): {len(labels[labels == 0])} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² ({len(labels[labels == 0])/len(labels)*100:.1f}%)\")\n",
    "print(f\"   â€¢ ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ (1): {len(labels[labels == 1])} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² ({len(labels[labels == 1])/len(labels)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "print(\"ðŸ“ˆ Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð Ð”ÐÐÐÐ«Ð¥ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð², Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²):\")\n",
    "display(features.iloc[:5, :10])\n",
    "\n",
    "print(\"\\nðŸ”¢ ÐžÐŸÐ˜Ð¡ÐÐ¢Ð•Ð›Ð¬ÐÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²):\")\n",
    "display(features.describe().iloc[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð­Ñ‚Ð°Ð¿ 2: ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "print(\"ðŸ”§ ÐŸÐ Ð•Ð”ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ Ð”ÐÐÐÐ«Ð¥\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "processed_features, processed_labels = diagnostic_system.preprocess_data(features, labels)\n",
    "\n",
    "print(f\"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹\")\n",
    "print(f\"ðŸ“ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð¿Ð¾ÑÐ»Ðµ Ð¾Ñ‚Ð±Ð¾Ñ€Ð°: {processed_features.shape[1]}\")\n",
    "print(f\"ðŸŽ¯ ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10):\")\n",
    "for i, feature in enumerate(processed_features.columns[:10], 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "if len(processed_features.columns) > 10:\n",
    "    print(f\"   ... Ð¸ ÐµÑ‰Ðµ {len(processed_features.columns) - 10} Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Random Forest\n",
    "print(\"ðŸŽ“ ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ÐœÐžÐ”Ð•Ð›Ð˜ RANDOM FOREST\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "training_results = diagnostic_system.train_model(processed_features, processed_labels)\n",
    "\n",
    "print(\"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð°!\")\n",
    "print()\n",
    "\n",
    "# Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ñ…\n",
    "diagnostic_system.print_diagnostic_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð­Ñ‚Ð°Ð¿ 3: Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ\n",
    "print(\"ðŸ”® Ð¢Ð•Ð¡Ð¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• ÐÐ ÐÐžÐ’Ð«Ð¥ Ð”ÐÐÐÐ«Ð¥\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "new_features, new_labels = diagnostic_system.generate_mimii_like_data(n_samples=500)\n",
    "\n",
    "# ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹\n",
    "predictions, probabilities = diagnostic_system.predict_anomaly(new_features)\n",
    "\n",
    "# ÐÐ½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²\n",
    "correct_predictions = np.sum(predictions == new_labels.values)\n",
    "accuracy_new_data = correct_predictions / len(predictions)\n",
    "\n",
    "print(f\"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(new_features)} Ð½Ð¾Ð²Ñ‹Ñ… Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²\")\n",
    "print(f\"ðŸŽ¯ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…: {accuracy_new_data:.4f}\")\n",
    "print()\n",
    "\n",
    "# Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(\"ðŸ“Š Ð ÐÐ¡ÐŸÐ Ð•Ð”Ð•Ð›Ð•ÐÐ˜Ð• ÐŸÐ Ð•Ð”Ð¡ÐšÐÐ—ÐÐÐ˜Ð™:\")\n",
    "for class_val, count in zip(unique, counts):\n",
    "    class_name = \"ÐÐ¾Ñ€Ð¼Ð°\" if class_val == 0 else \"ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ\"\n",
    "    percentage = (count / len(predictions)) * 100\n",
    "    print(f\"   â€¢ {class_name} (ÐºÐ»Ð°ÑÑ {int(class_val)}): {count} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² ({percentage:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# ÐÐ½Ð°Ð»Ð¸Ð· ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "high_confidence = np.sum((probabilities > 0.8) | (probabilities < 0.2))\n",
    "medium_confidence = np.sum((probabilities >= 0.2) & (probabilities <= 0.8))\n",
    "\n",
    "print(\"ðŸŽ² ÐÐÐÐ›Ð˜Ð— Ð£Ð’Ð•Ð Ð•ÐÐÐžÐ¡Ð¢Ð˜ ÐœÐžÐ”Ð•Ð›Ð˜:\")\n",
    "print(f\"   â€¢ Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ (>80% Ð¸Ð»Ð¸ <20%): {high_confidence} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² ({high_confidence/len(predictions)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Ð¡Ñ€ÐµÐ´Ð½ÑÑ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ (20-80%): {medium_confidence} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² ({medium_confidence/len(predictions)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\n",
    "print(\"ðŸ“‹ ÐŸÐ Ð˜ÐœÐ•Ð Ð« ÐŸÐ Ð•Ð”Ð¡ÐšÐÐ—ÐÐÐ˜Ð™ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10):\")\n",
    "print(\"â„–  | Ð˜ÑÑ‚Ð¸Ð½Ð½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ | ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ | Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ | Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(min(10, len(predictions))):\n",
    "    true_class = \"ÐÐ¾Ñ€Ð¼Ð°\" if new_labels.iloc[i] == 0 else \"ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ\"\n",
    "    pred_class = \"ÐÐ¾Ñ€Ð¼Ð°\" if predictions[i] == 0 else \"ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ\"\n",
    "    prob = probabilities[i]\n",
    "    confidence = \"Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ\" if prob > 0.8 or prob < 0.2 else \"Ð¡Ñ€ÐµÐ´Ð½ÑÑ\"\n",
    "    correct = \"âœ“\" if predictions[i] == new_labels.iloc[i] else \"âœ—\"\n",
    "    print(f\"{i+1:2d} | {true_class:13s} | {pred_class:12s} | {prob:17.4f} | {confidence:10s} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð­Ñ‚Ð°Ð¿ 4: Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Ð“Ñ€Ð°Ñ„Ð¸Ðº 1: Ð’Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
    "plt.subplot(2, 2, 1)\n",
    "top_features = diagnostic_system.get_feature_importance(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], \n",
    "         color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Ð’Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°')\n",
    "plt.title('Ð¢Ð¾Ð¿-10 Ð²Ð°Ð¶Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Ð“Ñ€Ð°Ñ„Ð¸Ðº 2: ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¾ÑˆÐ¸Ð±Ð¾Ðº\n",
    "plt.subplot(2, 2, 2)\n",
    "cm = diagnostic_system.training_history['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['ÐÐ¾Ñ€Ð¼Ð°', 'ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ'], \n",
    "            yticklabels=['ÐÐ¾Ñ€Ð¼Ð°', 'ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ'])\n",
    "plt.title('ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¾ÑˆÐ¸Ð±Ð¾Ðº')\n",
    "plt.ylabel('Ð˜ÑÑ‚Ð¸Ð½Ð½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ')\n",
    "plt.xlabel('ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ')\n",
    "\n",
    "# Ð“Ñ€Ð°Ñ„Ð¸Ðº 3: Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹\n",
    "plt.subplot(2, 2, 3)\n",
    "test_probs = diagnostic_system.training_history['test_probabilities']\n",
    "test_labels = diagnostic_system.training_history['test_labels']\n",
    "\n",
    "plt.hist(test_probs[test_labels == 0], bins=30, alpha=0.7, label='ÐÐ¾Ñ€Ð¼Ð°', color='blue')\n",
    "plt.hist(test_probs[test_labels == 1], bins=30, alpha=0.7, label='ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ñ', color='red')\n",
    "plt.xlabel('Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸')\n",
    "plt.ylabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²')\n",
    "plt.title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹')\n",
    "plt.legend()\n",
    "\n",
    "# Ð“Ñ€Ð°Ñ„Ð¸Ðº 4: ROC ÐºÑ€Ð¸Ð²Ð°Ñ\n",
    "plt.subplot(2, 2, 4)\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
    "auc_score = diagnostic_system.training_history['test_roc_auc']\n",
    "\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC ÐºÑ€Ð¸Ð²Ð°Ñ (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Ð¡Ð»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Ð›Ð¾Ð¶Ð½Ð¾Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð´Ð¾Ð»Ñ')\n",
    "plt.ylabel('Ð˜ÑÑ‚Ð¸Ð½Ð½Ð¾Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð´Ð¾Ð»Ñ')\n",
    "plt.title('ROC ÐºÑ€Ð¸Ð²Ð°Ñ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¾Ð² - ÐÐ½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²', \n",
    "             fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹\n",
    "\n",
    "### ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ:\n",
    "\n",
    "âœ… **Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ**: 99.9% Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…  \n",
    "âœ… **Ð˜Ð´ÐµÐ°Ð»ÑŒÐ½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ**: ROC-AUC = 1.000  \n",
    "âœ… **Ð¡Ñ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°**: F1-Ð¼ÐµÑ€Ð° 0.9991 Â± 0.0008 (ÐºÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ)  \n",
    "âœ… **Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ**: 98.8% Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ  \n",
    "\n",
    "### Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ:\n",
    "\n",
    "- **ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼**: Random Forest (100 Ð´ÐµÑ€ÐµÐ²ÑŒÐµÐ²) Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸\n",
    "- **ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°**: StandardScaler + SelectKBest (25 Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²)\n",
    "- **Ð‘Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ°**: ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
    "- **Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ**: 5-fold ÑÑ‚Ñ€Ð°Ñ‚Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÐºÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
    "\n",
    "### ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ:\n",
    "\n",
    "1. ðŸ­ **ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸** ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ñ‚Ñ€ÑƒÐ±Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ\n",
    "2. ðŸš¨ **Ð Ð°Ð½Ð½ÐµÐµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹** Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ Ð°Ð²Ð°Ñ€Ð¸Ð¹\n",
    "3. ðŸ“ˆ **ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð»Ð°Ð½Ð¾Ð² Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ** Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ\n",
    "4. ðŸ’° **Ð¡Ð½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚** Ð½Ð° Ð²Ð½ÐµÐ¿Ð»Ð°Ð½Ð¾Ð²Ñ‹Ðµ Ñ€ÐµÐ¼Ð¾Ð½Ñ‚Ñ‹\n",
    "5. ðŸ›¡ï¸ **ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸** Ð¿Ñ€Ð¾Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹\n",
    "\n",
      "### Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð²Ð½ÐµÐ´Ñ€ÐµÐ½Ð¸Ñ:\n",
    "\n",
    "- Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð´Ð°Ñ‚Ñ‡Ð¸ÐºÐ°Ð¼Ð¸ MIMII Ð¾Ñ‚ Hitachi\n",
    "- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ð¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð´ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ðµ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ\n",
    "- Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð°Ð»ÐµÑ€Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð²\n",
    "- Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾Ðµ Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "- ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð´Ñ€Ð¸Ñ„Ñ‚Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
